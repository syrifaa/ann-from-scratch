{"cells":[{"cell_type":"code","metadata":{"source_hash":"e23dd9a9","execution_start":1737428940049,"execution_millis":2878,"execution_context_id":"8ffdaa9e-bb70-4431-a2d6-0b621c42e9d8","deepnote_to_be_reexecuted":false,"cell_id":"c0f70fc67f8e4308aec5ebf26878d7c1","deepnote_cell_type":"code"},"source":"import csv\nimport json\nimport numpy as np\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder  # Add this import statement\nimport random\nimport pickle","block_group":"a0528e0550ff4cbaab14b0b334c6ecbd","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"5bc262a3","execution_start":1737428942981,"execution_millis":0,"execution_context_id":"8ffdaa9e-bb70-4431-a2d6-0b621c42e9d8","deepnote_to_be_reexecuted":false,"cell_id":"9845a291e2b444449bd2c3b542afb87f","deepnote_cell_type":"code"},"source":"class fileSystem:\n    @staticmethod\n    def readCSV(filename):\n        id_list = []\n        sepal_length_list = []\n        sepal_width_list = []\n        petal_length_list = []\n        petal_width_list = []\n        species_list = []\n        with open(filename + '.csv', 'r') as csv_file:\n            csv_reader = csv.reader(csv_file)\n            next(csv_reader)\n            for row in csv_reader:\n                id = row[0]\n                sepal_length = row[1].split(',')  # Split layers string into list\n                sepal_width = row[2].split(',')  \n                petal_length = row[3].split(',')\n                petal_width = row[4].split(',')\n                species = row[5].split(',')\n                \n                id_list.append(id)\n                sepal_length_list.append(sepal_length)\n                sepal_width_list.append(sepal_width)\n                petal_length_list.append(petal_length)\n                petal_width_list.append(petal_width)\n                species_list.append(species)\n\n        return id_list, sepal_length_list, sepal_width_list, petal_length_list, petal_width_list, species_list\n\n    def readJSON(filename):\n        with open(\"testcaseB/\" + filename + \".json\", 'r') as file:\n            data = json.load(file)\n            input_size = data[\"case\"][\"model\"][\"input_size\"]\n            layer = data[\"case\"][\"model\"][\"layers\"]\n            weights = data[\"case\"][\"initial_weights\"]\n            input_array = np.array(data[\"case\"][\"input\"])\n            target = np.array(data[\"case\"][\"target\"])\n            learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n            batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n            max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n            error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n            num_layer = len(layer)\n\n        return input_size, layer, weights, input_array, target, learning_rate, batch_size, max_iteration, error_threshold, num_layer","block_group":"edb4644fafa44e97993bce8f07ef39a8","execution_count":2,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"93f16555","execution_start":1737430641801,"execution_millis":1,"execution_context_id":"8ffdaa9e-bb70-4431-a2d6-0b621c42e9d8","deepnote_to_be_reexecuted":false,"cell_id":"a1ce023dbc3040749b2dbb6bd182bd9e","deepnote_cell_type":"code"},"source":"import numpy as np\n\nclass forwardActivation:\n    def relu(x):\n        return np.maximum(0, x)\n\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    def linear(x):\n        return x\n\n    def softmax(x):\n        exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n\nclass backActivationOutput:\n    def relu(x):\n        return 1 * (x >= 0)\n\n    def linear(x):\n        return np.ones_like(x)\n\n    def sigmoid(x):\n        return x * (1 - x)\n    \n    def softmax(x):\n        return x * (1 - x)\n\nclass backActivationHidden:\n    def relu(x, y):\n        return np.where(y < 0, 0, -(x - y))\n    \n    def linear(x, y):\n        return -(x - y)\n\n    def sigmoid(x, y):\n        return -y * (1 - y) * (x - y)\n    \n    def softmax(x, y):\n        return x * (1 - x)\n","block_group":"ec8b8a1a31674b8dabf84037bfc2b7df","execution_count":16,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a6330947","execution_start":1737432507246,"execution_millis":577,"execution_context_id":"8ffdaa9e-bb70-4431-a2d6-0b621c42e9d8","deepnote_to_be_reexecuted":false,"cell_id":"3043619614664cdb91ddd24becf1e713","deepnote_cell_type":"code"},"source":"class Layer:\n    def __init__(self, neurons, activation, weights=None, bias=1):\n        self.neurons = neurons\n        self.activation_name = activation\n        self.activation_func = forwardActivation.__dict__[activation]\n        self.activation_derivative_output = backActivationOutput.__dict__[activation]\n        self.activation_derivative_hidden = backActivationHidden.__dict__[activation]\n        self.weights = weights\n        self.bias = bias\n        self.outputs = None  # Initialize outputs attribute to None\n\n    def set_outputs(self, outputs):\n        self.outputs = outputs\n\nclass NeuralNetwork:\n    def __init__(self, learning_rate=0.1, batch_size=10, max_iterations=100, error_threshold=0.1):\n        self.layers = []\n        self.learning_rate = learning_rate\n        self.batch_size = batch_size\n        self.max_iteration = max_iterations\n        self.error_threshold = error_threshold\n\n    def add_layer(self, num_neurons, activation_function, weights, bias):\n        self.layers.append(Layer(\n            neurons=num_neurons,\n            activation=activation_function,\n            weights=weights,\n            bias=bias,\n        ))\n\n    def forward_pass(self, input_array):\n        outputs = input_array\n        for i, layer in enumerate(self.layers):\n            layer_output = layer.activation_func(np.dot(outputs, layer.weights) + layer.bias)\n            layer.set_outputs(layer_output)  # Set outputs attribute\n            outputs = layer_output\n        return outputs\n\n    def calculate_loss(self, outputs, targets):\n        return np.mean(np.square(targets - outputs))\n\n    def backward_pass(self, input_array, targets, outputs):\n        delta = -(targets - outputs) * self.layers[-1].activation_derivative_output(outputs)\n        for i in range(len(self.layers) - 1, -1, -1):\n            layer = self.layers[i]\n\n            if i != 0:\n                delta_weights = np.dot(self.layers[i - 1].outputs.T, delta)\n            else:\n                delta_weights = np.dot(input_array.T, delta)\n            layer.weights -= self.learning_rate * delta_weights\n            \n            delta_bias = np.sum(delta, axis=0)\n            layer.bias -= self.learning_rate * delta_bias    \n\n            if i > 0:\n                delta = np.dot(delta, self.layers[i].weights.T) * self.layers[i].activation_derivative_hidden(self.layers[i - 1].outputs, outputs)\n\n    def train(self, input_array, targets):\n        for epoch in range(self.max_iteration):\n            for i in range(0, len(input_array), self.batch_size):\n                input_batch = input_array[i:i + self.batch_size]\n                target_batch = targets[i:i + self.batch_size]\n\n                # Forward pass for the entire batch\n                outputs = self.forward_pass(input_batch)\n\n                # Backward pass for the entire batch\n                self.backward_pass(input_batch, target_batch, outputs)\n\n                # Calculate loss for the entire batch\n                loss = self.calculate_loss(outputs, target_batch)\n\n                print(f\"Iteration {epoch + 1}, Loss: {loss}\")\n\n                # Check for early stopping\n                if loss < self.error_threshold:\n                    print(\"Training converged: Error threshold reached.\")\n                    print(\"Final Weights:\")\n                    for i, layer in enumerate(self.layers):\n                        print(f\"Layer {i+1}:\")\n                        print(layer.bias)\n                        print(layer.weights)\n                    return\n                    \n            if epoch == self.max_iteration - 1:\n                print(\"Training stopped by max iteration.\")\n        \n        print(\"Final Weights:\")\n        for i, layer in enumerate(self.layers):\n            print(f\"Layer {i+1}:\")\n            print(layer.bias)\n            print(layer.weights)\n\n    def addLayer(self, layer: Layer, input_size=None) -> None:\n        if not self.layers:\n            if input_size is None:\n                raise ValueError(\"First layer must define input_size explicitly.\")\n            prev_neurons = input_size\n        else:\n            prev_neurons = self.layers[-1].neurons  \n        if layer.weights is None:\n            layer.weights = np.random.randn(prev_neurons, layer.neurons) * 0.01\n        if layer.bias is None:\n            layer.bias = np.zeros(layer.neurons)\n        self.layers.append(layer)\n\n\n    def save_model(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self, f)\n    \n    def load_model(filename):\n        with open(filename, 'rb') as f:\n            return pickle.load(f)\n    \n    def fit(self, input_array, targets):\n        for _ in range(self.max_iteration):\n            for i in range(0, len(input_array), self.batch_size):\n                input_batch = input_array[i:i + self.batch_size]\n                target_batch = targets[i:i + self.batch_size]\n\n                # Forward pass for the entire batch\n                outputs = self.forward_pass(input_batch)\n\n                # Backward pass for the entire batch\n                self.backward_pass(input_batch, target_batch, outputs)\n\n                # Calculate loss for the entire batch\n                loss = np.mean(np.square(target_batch - outputs))\n\n                if loss < self.error_threshold:\n                    return \"Training converged: Error threshold reached.\"\n\n        return \"Training stopped by max iteration.\"\n    \n    def predict(self, input_array):\n        outputs = self.forward_pass(input_array)\n        return outputs\n\n\nfile_name = 'linear'\ninput_size, layers, weights, input_array, targets, learning_rate, batch_size, max_iteration, error_threshold, num_layer = fileSystem.readJSON(file_name)\nnn = NeuralNetwork(learning_rate=learning_rate, batch_size=batch_size, max_iterations=max_iteration, error_threshold=error_threshold)\n\nfor layer, weight in zip(layers, weights):\n    nn.add_layer(layer[\"number_of_neurons\"], activation_function=layer[\"activation_function\"], weights=np.array(weight[1:]), bias=np.array(weight[0]))\n\nnn.train(input_array, targets)\nnn.save_model('model.pickle')\nloaded_nn = NeuralNetwork.load_model('model.pickle')\nprint(\"\\nLoaded Neural Network:\")\nfor i, layer in enumerate(loaded_nn.layers):\n    print(f\"Layer {i+1}:\")\n    print(f\"Neurons: {layer.neurons}\")\n    print(f\"Activation Function: {layer.activation_name}\")\n    print(f\"Weights:\")\n    print(layer.weights)\n    print(f\"Bias: {layer.bias}\")","block_group":"a867cdf59b404173b2c9702ea92abce9","execution_count":85,"outputs":[{"name":"stdout","text":"Iteration 1, Loss: 0.22166666666666668\nTraining stopped by max iteration.\nFinal Weights:\nLayer 1:\n[0.22 0.36 0.11]\n[[ 0.64  0.3  -0.89]\n [ 0.28 -0.7   0.37]]\n\nLoaded Neural Network:\nLayer 1:\nNeurons: 3\nActivation Function: linear\nWeights:\n[[ 0.64  0.3  -0.89]\n [ 0.28 -0.7   0.37]]\nBias: [0.22 0.36 0.11]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/58769b33-1667-4423-a258-a9a2f73f90e6","content_dependencies":null},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"f2699a34acec4e7c853ff38ec60ff10a","deepnote_cell_type":"text-cell-h1"},"source":"# Perbandingan dengan SKLearn","block_group":"d794a7c7b3134c24bc576f39c2a83c83"},{"cell_type":"code","metadata":{"source_hash":"dcd5b5ba","execution_start":1737432691944,"execution_millis":120,"execution_context_id":"8ffdaa9e-bb70-4431-a2d6-0b621c42e9d8","cell_id":"d9f223f5dd05408f91097dc9fc47e173","deepnote_cell_type":"code"},"source":"iris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\nX_train, X_val, y_train, y_val = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n\nmodel_iris_sklearn = MLPClassifier(hidden_layer_sizes=(4,), activation=\"logistic\", learning_rate_init=0.1,\n                                   batch_size=10, max_iter=1000, solver=\"sgd\", random_state=42)\nmodel_iris_sklearn.fit(X_train, y_train)\npredictions_sklearn = model_iris_sklearn.predict(X_val)\n\nreport_sklearn = classification_report(y_val, predictions_sklearn)\nprint(report_sklearn)","block_group":"94e608cf1a1447b39a532d5a95700629","execution_count":107,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       1.00      0.54      0.70        13\n           2       0.68      1.00      0.81        13\n\n    accuracy                           0.87        45\n   macro avg       0.89      0.85      0.84        45\nweighted avg       0.91      0.87      0.86        45\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ef1ef900-d9e1-4904-8d7f-af445145260e","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b83c58ce","execution_start":1737432640696,"execution_millis":1279,"execution_context_id":"8ffdaa9e-bb70-4431-a2d6-0b621c42e9d8","cell_id":"b53df1c826854a54b66ff16dcc14fe88","deepnote_cell_type":"code"},"source":"mlp_custom = NeuralNetwork(learning_rate=0.1, max_iterations=1000, batch_size=10, error_threshold=0.01)\nmlp_custom.addLayer(Layer(neurons=4, activation='sigmoid'), input_size=X_train.shape[1])  # Hidden layer\nmlp_custom.addLayer(Layer(neurons=y_train_onehot.shape[1], activation='softmax'))  # Output layer\ny_train_onehot = np.eye(3)[y_train]  # Convert target to one-hot encoding\nstop_reason = mlp_custom.fit(X_train, y_train_onehot)\n\npredictions_custom = np.argmax(mlp_custom.predict(X_val), axis=1)\n\nreport_custom = classification_report(y_val, predictions_custom)\nprint(report_custom)","block_group":"aabddc3946414801a56dc358ec543c28","execution_count":101,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        19\n           1       1.00      0.85      0.92        13\n           2       0.87      1.00      0.93        13\n\n    accuracy                           0.96        45\n   macro avg       0.96      0.95      0.95        45\nweighted avg       0.96      0.96      0.96        45\n\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/ae782e5a-3a1c-48dd-b7ca-81d9c0b51ac4","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=620db8da-30e5-4498-b1a7-3d24bc2b369d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-04-22T17:15:28.280Z"},"deepnote_notebook_id":"840e2a9183f2497ab24e0af9b5b29d9e"}}